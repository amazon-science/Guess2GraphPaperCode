"""
Real-World Data Experiment Plotting Module

Generate violin plots and other visualizations for real-world dataset experiments.

I think this only works for experiments generated by experiment_baseline_pc_sgs_guess_real_world_data, not the DAG file.
"""

import os
import json
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, List, Any, Optional
import pandas as pd
from collections import defaultdict


def load_experiment_data(experiment_dir: str) -> Dict[str, Any]:
    """Load experiment data from individual trial files."""
    config_path = os.path.join(experiment_dir, "config.json")
    
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"Config file not found: {config_path}")
    
    with open(config_path, 'r') as f:
        config = json.load(f)
    
    # Load results for each combination
    results = {}
    
    # Get all combination directories
    for item in os.listdir(experiment_dir):
        item_path = os.path.join(experiment_dir, item)
        if os.path.isdir(item_path) and item.startswith("dataset_"):
            # Load individual trial data
            trial_data = defaultdict(list)
            
            # Find all trial directories
            trial_dirs = [d for d in os.listdir(item_path) if d.startswith("trial_")]
            
            for trial_dir in sorted(trial_dirs):
                trial_path = os.path.join(item_path, trial_dir, "metrics.json")
                if os.path.exists(trial_path):
                    with open(trial_path, 'r') as f:
                        trial_metrics = json.load(f)
                    
                    # Extract metrics for each algorithm
                    for alg_name, metrics in trial_metrics.items():
                        if isinstance(metrics, dict) and 'f1' in metrics:
                            for metric_name, value in metrics.items():
                                trial_data[f"{alg_name}_{metric_name}"].append(value)
            
            results[item] = {
                'trial_data': trial_data,
                'combination_dir': item_path
            }
    
    return {
        'config': config,
        'results': results
    }


def extract_algorithm_data(experiment_data: Dict[str, Any], metric: str = 'f1') -> pd.DataFrame:
    """Extract algorithm performance data for plotting, preserving algorithm order."""
    rows = []
    
    for combo_key, combo_data in experiment_data['results'].items():
        trial_data = combo_data['trial_data']
        
        # Parse combo key to get dataset config
        parts = combo_key.split('_')
        dataset = parts[1]
        ci_test = parts[3]
        sample_size = int(parts[5])
        
        # Get all algorithms from trial data
        algorithms = set()
        for key in trial_data.keys():
            if key.endswith(f'_{metric}'):
                alg_name = key[:-len(f'_{metric}')]
                algorithms.add(alg_name)
        
        for algorithm in algorithms:
            metric_key = f"{algorithm}_{metric}"
            if metric_key in trial_data and trial_data[metric_key]:
                values = trial_data[metric_key]
                
                # Parse algorithm name and config
                if algorithm.startswith('pc_guess_expert_'):
                    config_idx = int(algorithm.split('_')[-1])
                    expert_config = experiment_data['config']['pc_guess_expert_configs'][config_idx]
                    algorithm_display = f"PC-Guess-Expert\n(edge={expert_config['p_acc_edge_true']}, MD={expert_config['p_acc_subset_MD']})"
                elif algorithm.startswith('sgs_guess_expert_'):
                    config_idx = int(algorithm.split('_')[-1])
                    expert_config = experiment_data['config']['sgs_guess_expert_configs'][config_idx]
                    algorithm_display = f"SGS-Guess-Expert\n(edge={expert_config['p_acc_edge_true']}, MD={expert_config['p_acc_subset_MD']})"
                else:
                    algorithm_display = algorithm.upper().replace('_', '-')
                
                rows.append({
                    'dataset': dataset,
                    'ci_test': ci_test,
                    'sample_size': sample_size,
                    'algorithm': algorithm,
                    'algorithm_display': algorithm_display,
                    'metric_mean': np.mean(values),
                    'metric_std': np.std(values),
                    'metric_min': np.min(values),
                    'metric_max': np.max(values),
                    'values': values
                })
    
    return pd.DataFrame(rows)


def create_metadata_box(fig, config: Dict[str, Any], additional_info: Optional[str] = None):
    """Add metadata text box at bottom of figure."""
    metadata_text = f"Experiment Info: "
    metadata_text += f"Date: {config.get('timestamp', 'N/A')[:10]} | "
    metadata_text += f"Trials: {config.get('num_trials', 'N/A')} | "
    metadata_text += f"Alpha: {config.get('alpha', 'N/A')} | "
    
    # Add dataset info
    datasets = config['real_world_params']['datasets']
    dataset_names = [d['dataset'] for d in datasets]
    metadata_text += f"Datasets: {dataset_names} | "
    
    # Add expert config info
    if 'pc_guess_expert_configs' in config:
        metadata_text += f"PC-Expert Configs: {len(config['pc_guess_expert_configs'])} | "
    if 'sgs_guess_expert_configs' in config:
        metadata_text += f"SGS-Expert Configs: {len(config['sgs_guess_expert_configs'])}"
    
    if additional_info:
        metadata_text += f" | {additional_info}"
    
    fig.text(0.5, 0.02, metadata_text, ha='center', va='bottom',
            bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray", alpha=0.8),
            fontsize=8, wrap=True)


def create_violin_plot(experiment_data: Dict[str, Any], metric: str = 'f1', 
                      save_path: Optional[str] = None, show_plot: bool = True) -> None:
    """Create violin plot comparing algorithms across datasets."""
    df = extract_algorithm_data(experiment_data, metric)
    
    if df.empty:
        print(f"No data found for metric: {metric}")
        return
    
    # Create figure
    fig, axes = plt.subplots(1, len(df['dataset'].unique()), 
                            figsize=(6 * len(df['dataset'].unique()), 8))
    
    if len(df['dataset'].unique()) == 1:
        axes = [axes]
    
    for i, dataset in enumerate(df['dataset'].unique()):
        dataset_df = df[df['dataset'] == dataset]
        
        # Preserve algorithm order from dataframe
        algorithm_order = dataset_df['algorithm_display'].tolist()
        
        # Create violin plot data using actual trial values
        plot_data = []
        for _, row in dataset_df.iterrows():
            for value in row['values']:
                plot_data.append({
                    'algorithm_display': row['algorithm_display'],
                    'value': value
                })
        
        plot_df = pd.DataFrame(plot_data)
        
        # Create violin plot with preserved order
        sns.violinplot(data=plot_df, x='algorithm_display', y='value', ax=axes[i], order=algorithm_order)
        
        # Customize plot
        axes[i].set_title(f"{dataset.upper()}\n(CI: {dataset_df.iloc[0]['ci_test']}, N: {dataset_df.iloc[0]['sample_size']})")
        axes[i].set_xlabel('Algorithm')
        axes[i].set_ylabel(f'{metric.upper()} Score')
        axes[i].tick_params(axis='x', rotation=45)
        axes[i].grid(True, alpha=0.3)
        
        # Set y-axis limits for common metrics
        if metric in ['precision', 'recall', 'f1']:
            axes[i].set_ylim(0, 1)
    
    plt.suptitle(metric.upper())
    plt.tight_layout()
    plt.subplots_adjust(bottom=0.2)  # Add space at bottom
    
    # Add metadata box at bottom
    create_metadata_box(fig, experiment_data['config'])
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"Plot saved to: {save_path}")
    
    if show_plot:
        plt.show()
    else:
        plt.close()


def create_comparison_table(experiment_data: Dict[str, Any], metrics: List[str] = None) -> pd.DataFrame:
    """Create comparison table of algorithm performance."""
    if metrics is None:
        metrics = ['f1', 'precision', 'recall']
    
    rows = []
    
    for combo_key, combo_data in experiment_data['results'].items():
        trial_data = combo_data['trial_data']
        
        # Parse combo key to get dataset config
        parts = combo_key.split('_')
        dataset = parts[1]
        ci_test = parts[3]
        sample_size = int(parts[5])
        
        # Get all algorithms from trial data
        algorithms = set()
        for key in trial_data.keys():
            for metric in metrics:
                if key.endswith(f'_{metric}'):
                    alg_name = key[:-len(f'_{metric}')]
                    algorithms.add(alg_name)
        
        for algorithm in algorithms:
            row = {
                'dataset': dataset,
                'ci_test': ci_test,
                'sample_size': sample_size,
                'algorithm': algorithm
            }
            
            # Add algorithm display name
            if algorithm.startswith('pc_guess_expert_'):
                config_idx = int(algorithm.split('_')[-1])
                expert_config = experiment_data['config']['pc_guess_expert_configs'][config_idx]
                row['algorithm_display'] = f"PC-Guess-Expert (edge={expert_config['p_acc_edge_true']}, MD={expert_config['p_acc_subset_MD']})"
            elif algorithm.startswith('sgs_guess_expert_'):
                config_idx = int(algorithm.split('_')[-1])
                expert_config = experiment_data['config']['sgs_guess_expert_configs'][config_idx]
                row['algorithm_display'] = f"SGS-Guess-Expert (edge={expert_config['p_acc_edge_true']}, MD={expert_config['p_acc_subset_MD']})"
            else:
                row['algorithm_display'] = algorithm.upper().replace('_', '-')
            
            # Add metrics
            for metric in metrics:
                metric_key = f"{algorithm}_{metric}"
                if metric_key in trial_data and trial_data[metric_key]:
                    values = trial_data[metric_key]
                    row[f'{metric}_mean'] = np.mean(values)
                    row[f'{metric}_std'] = np.std(values)
            
            rows.append(row)
    
    return pd.DataFrame(rows)


def plot_violin_metric(experiment_dir: str, metric: str, output_path: Optional[str] = None):
    """Create violin plot for a single metric."""
    # Load data
    experiment_data = load_experiment_data(experiment_dir)
    
    if not experiment_data['results']:
        print("No results found in experiment directory")
        return
    
    # Generate output path if not provided
    if output_path is None:
        current_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(current_dir)
        plots_dir = os.path.join(project_root, "plots")
        os.makedirs(plots_dir, exist_ok=True)
        
        exp_num = os.path.basename(experiment_dir).split('_')[-1]
        output_path = os.path.join(plots_dir, f"exp{exp_num}_real_world_{metric}.png")
    
    # Create and save plot
    create_violin_plot(experiment_data, metric, save_path=output_path, show_plot=True)


def plot_experiment_results(experiment_dir: str, metrics: List[str] = None, 
                          output_dir: Optional[str] = None) -> None:
    """Generate all plots for an experiment."""
    if metrics is None:
        metrics = ['f1', 'precision', 'recall']
    
    # Load experiment data
    experiment_data = load_experiment_data(experiment_dir)
    
    if not experiment_data['results']:
        print("No results found in experiment directory")
        return
    
    # Create output directory in global plots folder like expert_plots
    if output_dir is None:
        current_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(current_dir)
        plots_dir = os.path.join(project_root, "plots")
        os.makedirs(plots_dir, exist_ok=True)
        
        exp_num = os.path.basename(experiment_dir).split('_')[-1]
        output_dir = plots_dir
    
    # Generate plots for each metric
    exp_num = os.path.basename(experiment_dir).split('_')[-1]
    for metric in metrics:
        plot_path = os.path.join(output_dir, f"exp{exp_num}_real_world_{metric}_comparison.png")
        create_violin_plot(experiment_data, metric, save_path=plot_path, show_plot=False)
    
    # Generate comparison table
    table_df = create_comparison_table(experiment_data, metrics)
    table_path = os.path.join(output_dir, f"exp{exp_num}_real_world_results_table.csv")
    table_df.to_csv(table_path, index=False)
    print(f"Results table saved to: {table_path}")
    
    print(f"All plots saved to: {output_dir}")


def plot_parameter_sweep(experiment_dir: str, metric: str, parameter: str, 
                        dataset: str, ci_test: str, sample_size: int, 
                        output_path: Optional[str] = None):
    """Plot parameter sweep for real-world experiments showing parameter-varying algorithms as points and parameter-invariant as horizontal lines."""
    # Load data
    experiment_data = load_experiment_data(experiment_dir)
    config = experiment_data['config']
    
    # Find the specific combination
    combo_key = f"dataset_{dataset}_ci_{ci_test}_samples_{sample_size}"
    
    if combo_key not in experiment_data['results']:
        print(f"No data found for dataset={dataset}, ci_test={ci_test}, sample_size={sample_size}")
        return
    
    combo_data = experiment_data['results'][combo_key]
    trial_data = combo_data['trial_data']
    
    # Map parameter name
    if parameter == "p_edge_acc":
        param_key = "p_acc_edge_true"  # Assuming p_acc_edge_true == p_acc_edge_false
    else:
        param_key = parameter
    
    # Create figure
    fig, ax = plt.subplots(1, 1, figsize=(10, 6))
    
    # Get all algorithms from trial data
    algorithms = set()
    for key in trial_data.keys():
        if key.endswith(f'_{metric}'):
            alg_name = key[:-len(f'_{metric}')]
            algorithms.add(alg_name)
    
    algorithms = list(algorithms)
    
    # Separate parameter-varying and parameter-invariant algorithms
    param_varying = []
    param_invariant = []
    
    for alg in algorithms:
        if alg.startswith('pc_guess_expert_') or alg.startswith('sgs_guess_expert_'):
            param_varying.append(alg)
        else:
            param_invariant.append(alg)
    
    # Get base algorithm types for coloring
    base_algorithms = set()
    for alg in algorithms:
        if alg.startswith('pc_guess_expert_'):
            base_algorithms.add('pc_guess_expert')
        elif alg.startswith('sgs_guess_expert_'):
            base_algorithms.add('sgs_guess_expert')
        else:
            base_algorithms.add(alg)
    
    # Create color palette for base algorithms
    base_colors = sns.color_palette("Set1", len(base_algorithms))
    color_map = {base_alg: base_colors[i] for i, base_alg in enumerate(sorted(base_algorithms))}
    
    # Track which legend entries have been added
    legend_added = set()
    
    # Plot parameter-invariant algorithms as horizontal dotted lines
    for alg in param_invariant:
        metric_key = f"{alg}_{metric}"
        if metric_key in trial_data and trial_data[metric_key]:
            alg_mean = np.mean(trial_data[metric_key])
            
            # Add label only to first occurrence of each algorithm type
            if alg not in legend_added:
                ax.axhline(y=alg_mean, color=color_map[alg], linestyle='--', alpha=0.7,
                          label=f"{alg.upper()} (invariant)")
                legend_added.add(alg)
            else:
                ax.axhline(y=alg_mean, color=color_map[alg], linestyle='--', alpha=0.7)
    
    # Plot parameter-varying algorithms as points
    for alg in param_varying:
        metric_key = f"{alg}_{metric}"
        if metric_key not in trial_data or not trial_data[metric_key]:
            continue
            
        # Get parameter value from config
        if alg.startswith('pc_guess_expert_'):
            config_idx = int(alg.split('_')[-1])
            param_value = config['pc_guess_expert_configs'][config_idx][param_key]
            base_alg = 'pc_guess_expert'
        elif alg.startswith('sgs_guess_expert_'):
            config_idx = int(alg.split('_')[-1])
            param_value = config['sgs_guess_expert_configs'][config_idx][param_key]
            base_alg = 'sgs_guess_expert'
        
        alg_mean = np.mean(trial_data[metric_key])
        
        # Add label only to first occurrence of each base algorithm type
        if base_alg not in legend_added:
            ax.scatter(param_value, alg_mean, color=color_map[base_alg], s=100, alpha=0.8,
                      label=f"{base_alg.upper().replace('_', '-')} (varying)")
            legend_added.add(base_alg)
        else:
            ax.scatter(param_value, alg_mean, color=color_map[base_alg], s=100, alpha=0.8)
    
    # Connect points of same base algorithm with lines
    for base_alg in ['pc_guess_expert', 'sgs_guess_expert']:
        if base_alg in [alg.split('_')[0] + '_' + alg.split('_')[1] + '_' + alg.split('_')[2] 
                       for alg in param_varying]:
            base_algs = [alg for alg in param_varying if alg.startswith(base_alg)]
            if len(base_algs) > 1:
                x_vals = []
                y_vals = []
                for alg in sorted(base_algs):
                    metric_key = f"{alg}_{metric}"
                    if metric_key not in trial_data or not trial_data[metric_key]:
                        continue
                    config_idx = int(alg.split('_')[-1])
                    if base_alg == 'pc_guess_expert':
                        param_value = config['pc_guess_expert_configs'][config_idx][param_key]
                    else:
                        param_value = config['sgs_guess_expert_configs'][config_idx][param_key]
                    alg_mean = np.mean(trial_data[metric_key])
                    x_vals.append(param_value)
                    y_vals.append(alg_mean)
                
                if len(x_vals) > 1:
                    ax.plot(x_vals, y_vals, color=color_map[base_alg], alpha=0.5, linewidth=1)
    
    ax.set_xlabel(f"{parameter.upper()}")
    ax.set_ylabel(f"{metric.upper()} (Mean)")
    ax.set_title(f"{metric.upper()} vs {parameter.upper()}")
    ax.grid(True, alpha=0.3)
    ax.legend()
    
    plt.tight_layout()
    plt.subplots_adjust(bottom=0.2)
    
    # Add metadata box
    additional_info = f"Setting: dataset={dataset}, ci_test={ci_test}, sample_size={sample_size}, parameter={parameter}"
    create_metadata_box(fig, config, additional_info)
    
    # Generate output path if not provided
    if output_path is None:
        current_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(current_dir)
        plots_dir = os.path.join(project_root, "plots")
        os.makedirs(plots_dir, exist_ok=True)
        
        exp_num = os.path.basename(experiment_dir).split('_')[-1]
        output_path = os.path.join(plots_dir, f"exp{exp_num}_real_world_parameter_sweep_{metric}_{parameter}_{dataset}_{ci_test}_{sample_size}.png")
    
    # Save and show
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"Plot saved to: {output_path}")
    plt.show()


def load_dag_experiment_data(experiment_dir: str) -> Dict[str, Any]:
    """Load DAG experiment data from experiment directory."""
    # Load config
    config_path = os.path.join(experiment_dir, "config.json")
    with open(config_path, 'r') as f:
        config = json.load(f)
    
    # Load results for each dataset combination
    results = {}
    
    for dataset_config in config['real_world_params']['datasets']:
        dataset_name = dataset_config['dataset']
        ci_test = dataset_config['ci_test']
        sample_size = dataset_config['sample_size']
        
        combo_key = f"dataset_{dataset_name}_ci_{ci_test}_samples_{sample_size}"
        combo_dir = os.path.join(experiment_dir, combo_key)
        
        if not os.path.exists(combo_dir):
            continue
            
        # Load aggregated metrics
        metrics_file = os.path.join(combo_dir, "aggregated_metrics.json")
        if os.path.exists(metrics_file):
            with open(metrics_file, 'r') as f:
                aggregated_metrics = json.load(f)
            
            # Load individual trial data for violin plots
            trial_data = {}
            for trial_idx in range(config['num_trials']):
                trial_dir = os.path.join(combo_dir, f"trial_{trial_idx:03d}")
                trial_metrics_file = os.path.join(trial_dir, "metrics.json")
                
                if os.path.exists(trial_metrics_file):
                    with open(trial_metrics_file, 'r') as f:
                        trial_metrics = json.load(f)
                    
                    # Extract algorithm results
                    for key, value in trial_metrics.items():
                        if key not in ["seed", "dataset", "ci_test", "sample_size", "variable_names", "methods", "runtimes"]:
                            if isinstance(value, dict):
                                for metric_name, metric_value in value.items():
                                    trial_key = f"{key}_{metric_name}"
                                    if trial_key not in trial_data:
                                        trial_data[trial_key] = []
                                    trial_data[trial_key].append(metric_value)
            
            results[combo_key] = {
                'aggregated_metrics': aggregated_metrics,
                'trial_data': trial_data,
                'config': dataset_config
            }
    
    return {
        'config': config,
        'results': results
    }


def create_dag_violin_plot(experiment_data: Dict[str, Any], metric: str, 
                           dataset: str, ci_test: str, sample_size: int,
                           save_path: Optional[str] = None, show_plot: bool = True):
    """Create violin plot for DAG experiment results."""
    combo_key = f"dataset_{dataset}_ci_{ci_test}_samples_{sample_size}"
    
    if combo_key not in experiment_data['results']:
        print(f"No data found for {combo_key}")
        return
    
    combo_data = experiment_data['results'][combo_key]
    trial_data = combo_data['trial_data']
    
    # Group algorithms by method+model combination
    method_model_data = {}
    baseline_data = {}
    
    # Process trial data
    for key, values in trial_data.items():
        if key.endswith(f'_{metric}'):
            alg_name = key[:-len(f'_{metric}')]
            
            if alg_name in ['pc', 'stable_pc', 'pc_guess_dag_true', 'sgs_guess_dag_true']:
                baseline_data[alg_name] = values
            else:
                # Parse model_method_dagindex -> model_method
                parts = alg_name.split('_')
                if len(parts) >= 3:
                    model_method = '_'.join(parts[:-1])
                    if model_method not in method_model_data:
                        method_model_data[model_method] = []
                    method_model_data[model_method].extend(values)
    
    # Prepare data for plotting
    plot_data = []
    labels = []
    colors = []
    
    # Add baseline methods
    baseline_color = 'lightblue'
    for alg_name in sorted(baseline_data.keys()):
        plot_data.append(baseline_data[alg_name])
        labels.append(alg_name.upper())
        colors.append(baseline_color)
    
    # Add model+method combinations
    model_method_colors = sns.color_palette("Set2", len(method_model_data))
    for i, (model_method, values) in enumerate(sorted(method_model_data.items())):
        plot_data.append(values)
        labels.append(model_method.upper().replace('_', '-'))
        colors.append(model_method_colors[i])
    
    if not plot_data:
        print(f"No data found for metric {metric}")
        return
    
    # Create violin plot
    fig, ax = plt.subplots(1, 1, figsize=(max(12, len(labels) * 0.8), 8))
    
    parts = ax.violinplot(plot_data, positions=range(len(plot_data)), showmeans=True, showmedians=True)
    
    # Color the violins
    for i, pc in enumerate(parts['bodies']):
        pc.set_facecolor(colors[i])
        pc.set_alpha(0.7)
    
    # Customize plot
    ax.set_xticks(range(len(labels)))
    ax.set_xticklabels(labels, rotation=45, ha='right')
    ax.set_ylabel(f"{metric.upper()}")
    ax.set_title(f"{metric.upper()} Distribution - {dataset} (CI: {ci_test}, Samples: {sample_size})")
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # Add metadata
    config = experiment_data['config']
    exp_num = os.path.basename(save_path).split('_')[0].replace('exp', '') if save_path else 'N/A'
    metadata_text = f"Experiment: {exp_num} | Trials: {config['num_trials']} | Alpha: {config['alpha']}"
    plt.figtext(0.02, 0.02, metadata_text, fontsize=8, style='italic')
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"Plot saved to: {save_path}")
    
    if show_plot:
        plt.show()
    else:
        plt.close()


def plot_dag_experiment_results(experiment_dir: str, output_dir: Optional[str] = None):
    """Generate all plots for a DAG experiment."""
    # Load experiment data
    experiment_data = load_dag_experiment_data(experiment_dir)
    config = experiment_data['config']
    
    if not experiment_data['results']:
        print("No results found in experiment directory")
        return
    
    # Set output directory
    if output_dir is None:
        current_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(current_dir)
        plots_dir = os.path.join(project_root, "plots")
        os.makedirs(plots_dir, exist_ok=True)
        output_dir = plots_dir
    
    exp_num = os.path.basename(experiment_dir).split('_')[-1]
    
    # Generate plots for each dataset combination and metric
    for combo_key, combo_data in experiment_data['results'].items():
        dataset_config = combo_data['config']
        dataset_name = dataset_config['dataset']
        ci_test = dataset_config['ci_test']
        sample_size = dataset_config['sample_size']
        
        for metric in config['metrics']:
            plot_path = os.path.join(output_dir, f"exp{exp_num}_dag_{dataset_name}_{ci_test}_{sample_size}_{metric}_violin.png")
            create_dag_violin_plot(experiment_data, metric, dataset_name, ci_test, sample_size, 
                                 save_path=plot_path, show_plot=False)
    
    print(f"All plots saved to: {output_dir}")


# Example usage
if __name__ == "__main__":
    # Example usage - update experiment_dir with your actual experiment number
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(current_dir)
    experiment_dir = os.path.join(project_root, "results", "experiment_399")
    
    print(f"Looking for experiment at: {experiment_dir}")
    
    # Show where plots will be saved
    plots_dir = os.path.join(project_root, "plots")
    print(f"Plots will be saved to: {plots_dir}")
    
    if not os.path.exists(experiment_dir):
        print(f"ERROR: Experiment directory does not exist: {experiment_dir}")
        print("Available experiments:")
        results_dir = os.path.join(project_root, "results")
        if os.path.exists(results_dir):
            for exp in sorted(os.listdir(results_dir)):
                if exp.startswith("experiment_"):
                    print(f"  {exp}")
        exit()
    
    # Test DAG experiment plotting
    print("Generating DAG experiment plots...")
    
    # Plot single metric
    experiment_data = load_dag_experiment_data(experiment_dir)
    create_dag_violin_plot(experiment_data, "f1", "sachs", "chisq", 100, show_plot=True)
    
    # Generate all plots
    plot_dag_experiment_results(experiment_dir)
    
    print("DAG experiment plots completed.")