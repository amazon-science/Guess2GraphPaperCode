"""Real-World Data DAG Experiment Plotting Module

Generate violin plots and other visualizations for real-world dataset experiments
generated by experiment_baseline_pc_sgs_dag_real_world_data.py.
"""

import os
import json
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import pandas as pd
from collections import defaultdict


def load_experiment_data(experiment_dir: str) -> Dict[str, Any]:
    """Load experiment data from individual trial files for DAG experiments."""
    config_path = os.path.join(experiment_dir, "config.json")
    
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"Config file not found: {config_path}")
    
    with open(config_path, 'r') as f:
        config = json.load(f)
    
    # Load LLM guess accuracy data
    llm_guess_data = _load_llm_guess_accuracy(config)
    print(f"DEBUG: Loaded LLM guess data for models: {list(llm_guess_data.keys())}")
    
    # Load results for each combination
    results = {}
    
    # Get all combination directories
    for item in os.listdir(experiment_dir):
        item_path = os.path.join(experiment_dir, item)
        if os.path.isdir(item_path) and item.startswith("dataset_"):
            # Load individual trial data
            trial_data = defaultdict(list)
            
            # Find all trial directories
            trial_dirs = [d for d in os.listdir(item_path) if d.startswith("trial_")]
            
            for trial_dir in sorted(trial_dirs):
                trial_path = os.path.join(item_path, trial_dir, "metrics.json")
                if os.path.exists(trial_path):
                    with open(trial_path, 'r') as f:
                        trial_metrics = json.load(f)
                    
                    # Extract metrics for each algorithm
                    for alg_name, metrics in trial_metrics.items():
                        if isinstance(metrics, dict) and any(m in metrics for m in ['f1', 'precision', 'recall']):
                            for metric_name, value in metrics.items():
                                trial_data[f"{alg_name}_{metric_name}"].append(value)
            
            # Add LLM guess accuracy data
            for model_name, guess_metrics in llm_guess_data.items():
                for metric_name, values in guess_metrics.items():
                    if values:  # Only add if there are values
                        trial_data[f"{model_name}_guess_{metric_name}"] = values
                        print(f"DEBUG: Added {len(values)} {metric_name} values for {model_name}_guess_{metric_name}")
                    else:
                        print(f"DEBUG: No {metric_name} values for {model_name}")
            
            results[item] = {
                'trial_data': trial_data,
                'combination_dir': item_path
            }
    
    return {
        'config': config,
        'results': results
    }


def _load_llm_guess_accuracy(config: Dict[str, Any]) -> Dict[str, Dict[str, List[float]]]:
    """Load LLM guess accuracy metrics from guess experiment folder."""
    guess_folder = config.get('guess_experiment_folder')
    print(f"DEBUG: Loading LLM guess data from folder: {guess_folder}")
    
    if not guess_folder:
        print("DEBUG: No guess_experiment_folder in config")
        return {}
    
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    analysis_path = os.path.join(project_root, "guess-llms-results", guess_folder, "analysis_results.json")
    print(f"DEBUG: Looking for analysis file at: {analysis_path}")
    
    if not os.path.exists(analysis_path):
        print(f"DEBUG: Analysis file does not exist: {analysis_path}")
        return {}
    
    with open(analysis_path, 'r') as f:
        analysis_data = json.load(f)
    
    print(f"DEBUG: Analysis data keys: {list(analysis_data.keys())}")
    print(f"DEBUG: Models in analysis: {list(analysis_data.get('models', {}).keys())}")
    
    llm_guess_data = {}
    for model_name in config.get('model_names', []):
        clean_model_name = model_name.replace('_results.json', '')
        print(f"DEBUG: Processing model: {model_name} -> {clean_model_name}")
        
        if clean_model_name in analysis_data.get('models', {}):
            model_data = analysis_data['models'][clean_model_name]
            skeleton_metrics = model_data.get('skeleton_metrics', {})
            
            llm_guess_data[clean_model_name] = {
                'precision': skeleton_metrics.get('precision', []),
                'recall': skeleton_metrics.get('recall', []),
                'f1': skeleton_metrics.get('f1', [])
            }
            print(f"DEBUG: Loaded {len(skeleton_metrics.get('f1', []))} F1 values for {clean_model_name}")
        else:
            print(f"DEBUG: Model {clean_model_name} not found in analysis data")
    
    print(f"DEBUG: Final LLM guess data keys: {list(llm_guess_data.keys())}")
    return llm_guess_data


def extract_algorithm_data(experiment_data: Dict[str, Any], metric: str = 'f1') -> pd.DataFrame:
    """Extract algorithm performance data for plotting DAG experiments."""
    rows = []
    
    for combo_key, combo_data in experiment_data['results'].items():
        trial_data = combo_data['trial_data']
        
        # Parse combo key to get dataset config
        parts = combo_key.split('_')
        dataset = parts[1]
        ci_test = parts[3]
        sample_size = int(parts[5])
        
        # Get all algorithms from trial data
        algorithms = set()
        for key in trial_data.keys():
            if key.endswith(f'_{metric}'):
                alg_name = key[:-len(f'_{metric}')]
                algorithms.add(alg_name)
        
        for algorithm in algorithms:
            metric_key = f"{algorithm}_{metric}"
            if metric_key in trial_data and trial_data[metric_key]:
                values = trial_data[metric_key]
                
                # Parse algorithm name and create display name
                algorithm_display, category = _get_algorithm_display_name(algorithm, experiment_data['config'])
                
                rows.append({
                    'dataset': dataset,
                    'ci_test': ci_test,
                    'sample_size': sample_size,
                    'algorithm': algorithm,
                    'algorithm_display': algorithm_display,
                    'is_baseline': category == 'baseline',
                    'is_guess': category == 'llm_guess',
                    'category': category,
                    'metric_mean': np.mean(values),
                    'metric_std': np.std(values),
                    'metric_min': np.min(values),
                    'metric_max': np.max(values),
                    'values': values
                })
    
    return pd.DataFrame(rows)


def _get_algorithm_display_name(algorithm: str, config: Dict[str, Any]) -> Tuple[str, str]:
    """Get display name and algorithm type for algorithm."""
    # Algorithm display name mappings
    if algorithm == 'stable_pc':
        return 'PC-Stable', 'baseline'
    elif algorithm == 'sgs_guess_expert':
        return 'gPC', 'expert'
    elif algorithm == 'claude_opus_4_1_guess':
        return 'Claude Opus 4.1', 'llm_guess'
    elif algorithm == 'claude_opus_4_1_sgs_guess_dag':
        return 'gPC-Guess +\nClaude Opus 4.1', 'combined'
    elif algorithm == 'pc':
        return 'PC', 'baseline'
    elif algorithm == 'pc_guess_expert':
        return 'PC-GUESS-R', 'expert'
    elif algorithm == 'claude_opus_4_1_pc_guess_dag':
        return 'PC-GUESS +\nClaude Opus 4.1', 'combined'
    
    # LLM guess accuracy patterns
    elif '_guess_f1' in algorithm or '_guess_precision' in algorithm or '_guess_recall' in algorithm:
        model_name = algorithm.split('_guess_')[0]
        if 'claude_opus_4_1' in model_name:
            return 'Claude Opus 4.1', 'llm_guess'
    
    # Fallback
    return algorithm.upper().replace('_', '-'), 'other'


def create_metadata_box(fig, config: Dict[str, Any], additional_info: Optional[str] = None):
    """Add metadata text box at bottom of figure for DAG experiments."""
    metadata_text = f"Experiment Info: "
    metadata_text += f"Date: {config.get('timestamp', 'N/A')[:10]} | "
    metadata_text += f"Alpha: {config.get('alpha', 'N/A')} | "
    
    # Add dataset info
    datasets = config['real_world_params']['datasets']
    dataset_names = [d['dataset'] for d in datasets]
    metadata_text += f"Datasets: {dataset_names} | "
    
    # Add LLM info
    if 'model_names' in config:
        models = [m.replace('_results.json', '') for m in config['model_names']]
        metadata_text += f"Models: {models} | "
    
    # Add expert params info
    if 'expert_params' in config:
        expert_params = config['expert_params']
        metadata_text += f"Expert: edge={expert_params.get('p_acc_edge_true', 'N/A')}, MD={expert_params.get('p_acc_subset_MD', 'N/A')}"
    
    if additional_info:
        metadata_text += f" | {additional_info}"
    
    fig.text(0.5, 0.02, metadata_text, ha='center', va='bottom',
            bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray", alpha=0.8),
            fontsize=8, wrap=True)


def create_violin_plot(experiment_data: Dict[str, Any], metric: str = 'f1', 
                      methods_to_plot: Optional[List[str]] = None,
                      save_path: Optional[str] = None, show_plot: bool = True) -> None:
    """Create violin plot comparing algorithms across datasets for DAG experiments."""
    df = extract_algorithm_data(experiment_data, metric)
    
    if df.empty:
        print(f"No data found for metric: {metric}")
        return
    
    # Filter methods if specified
    if methods_to_plot:
        print(f"DEBUG: Filtering for methods: {methods_to_plot}")
        print(f"DEBUG: Available algorithms: {df['algorithm'].unique().tolist()}")
        df = df[df['algorithm'].isin(methods_to_plot)]
        if df.empty:
            print(f"No data found for specified methods: {methods_to_plot}")
            return
        print(f"DEBUG: After filtering, found algorithms: {df['algorithm'].unique().tolist()}")
    
    # Create figure with reduced height
    fig, axes = plt.subplots(1, len(df['dataset'].unique()), 
                            figsize=(6 * len(df['dataset'].unique()), 4))
    
    if len(df['dataset'].unique()) == 1:
        axes = [axes]
    
    for i, dataset in enumerate(df['dataset'].unique()):
        dataset_df = df[df['dataset'] == dataset]
        
        # Custom algorithm ordering (top to bottom for horizontal plot)
        algorithm_order = ['gPC-Guess +\nClaude Opus 4.1', 'gPC', 'Claude Opus 4.1', 'PC-Stable']
        
        if methods_to_plot:
            # Filter to only requested methods and maintain order
            filtered_df = dataset_df[dataset_df['algorithm'].isin(methods_to_plot)]
            # Create order mapping based on display names
            order_map = {name: i for i, name in enumerate(algorithm_order)}
            filtered_df = filtered_df.copy()
            filtered_df['order'] = filtered_df['algorithm_display'].map(order_map)
            sorted_df = filtered_df.sort_values('order')
        else:
            # Use default algorithm order
            order_map = {name: i for i, name in enumerate(algorithm_order)}
            dataset_df = dataset_df.copy()
            dataset_df['order'] = dataset_df['algorithm_display'].map(order_map)
            sorted_df = dataset_df.sort_values('order')
        
        algorithm_order = sorted_df['algorithm_display'].tolist()
        
        # Create violin plot data using actual trial values
        plot_data = []
        for _, row in sorted_df.iterrows():
            for value in row['values']:
                plot_data.append({
                    'algorithm_display': row['algorithm_display'],
                    'value': value,
                    'category': row['category']
                })
        
        plot_df = pd.DataFrame(plot_data)
        
        # Create horizontal boxplot with algorithm-specific colors
        def get_algorithm_color(algorithm_display):
            if algorithm_display == 'PC-Stable':
                return '#808080'  # Light black/dark gray
            elif algorithm_display == 'Claude Opus 4.1':
                return '#FFB366'  # Light orange (closer to actual orange)
            elif algorithm_display == 'gPC':
                return '#DDA0DD'  # Light purple
            elif algorithm_display == 'gPC-Guess +\nClaude Opus 4.1':
                return '#DDA0DD'  # Will be handled specially with stripes
            else:
                return 'lightgray'
        
        # Create horizontal boxplot with much tighter spacing
        positions = [i * 0.25 for i in range(len(sorted_df))]  # Much closer spacing
        box_parts = axes[i].boxplot([row['values'] for _, row in sorted_df.iterrows()], 
                                   positions=positions,
                                   vert=False,  # Make horizontal
                                   patch_artist=True,  # Enable color filling
                                   medianprops={'color': 'red', 'linewidth': 2},  # Bold red median
                                   widths=0.15)  # Very narrow boxes for tight spacing
        
        # Apply colors and special striped pattern for combined method
        for j, (_, row) in enumerate(sorted_df.iterrows()):
            color = get_algorithm_color(row['algorithm_display'])
            
            if row['algorithm_display'] == 'gPC-Guess +\nClaude Opus 4.1':
                # Apply striped pattern (purple and orange)
                box_parts['boxes'][j].set_facecolor('#DDA0DD')
                box_parts['boxes'][j].set_alpha(0.7)
                # Add orange stripes
                box_parts['boxes'][j].set_hatch('///')
                box_parts['boxes'][j].set_edgecolor('#FFB366')
            else:
                box_parts['boxes'][j].set_facecolor(color)
                box_parts['boxes'][j].set_alpha(0.7)
        
        # Set y-axis labels (algorithms are now on y-axis)
        axes[i].set_yticks(positions)
        axes[i].set_yticklabels(sorted_df['algorithm_display'].tolist())
        
        # Set tight y-axis limits to remove extra whitespace
        axes[i].set_ylim(min(positions) - 0.1, max(positions) + 0.1)
        axes[i].margins(y=0)
        
        # Remove printed median values
        
        # Customize plot
        axes[i].set_xlabel(f'{metric.upper()} Score')
        axes[i].set_ylabel('Methods', rotation=90, va='bottom')
        axes[i].grid(True, alpha=0.3)
        
        # Set x-axis limits for common metrics (now horizontal)
        if metric in ['precision', 'recall', 'f1']:
            axes[i].set_xlim(0.35, 0.9)
    
    # Remove suptitle for cleaner appearance
    plt.tight_layout(pad=0.5)
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"Plot saved to: {save_path}")
    
    if show_plot:
        plt.show()
    else:
        plt.close()


def create_comparison_table(experiment_data: Dict[str, Any], metrics: List[str] = None) -> pd.DataFrame:
    """Create comparison table of algorithm performance."""
    if metrics is None:
        metrics = ['f1', 'precision', 'recall']
    
    rows = []
    
    for combo_key, combo_data in experiment_data['results'].items():
        trial_data = combo_data['trial_data']
        
        # Parse combo key to get dataset config
        parts = combo_key.split('_')
        dataset = parts[1]
        ci_test = parts[3]
        sample_size = int(parts[5])
        
        # Get all algorithms from trial data
        algorithms = set()
        for key in trial_data.keys():
            for metric in metrics:
                if key.endswith(f'_{metric}'):
                    alg_name = key[:-len(f'_{metric}')]
                    algorithms.add(alg_name)
        
        for algorithm in algorithms:
            row = {
                'dataset': dataset,
                'ci_test': ci_test,
                'sample_size': sample_size,
                'algorithm': algorithm
            }
            
            # Add algorithm display name
            algorithm_display, category = _get_algorithm_display_name(algorithm, experiment_data['config'])
            row['algorithm_display'] = algorithm_display.replace('\n', ' ')
            row['is_baseline'] = category == 'baseline'
            row['is_guess'] = category == 'llm_guess'
            row['category'] = category
            
            # Add metrics
            for metric in metrics:
                metric_key = f"{algorithm}_{metric}"
                if metric_key in trial_data and trial_data[metric_key]:
                    values = trial_data[metric_key]
                    row[f'{metric}_mean'] = np.mean(values)
                    row[f'{metric}_std'] = np.std(values)
            
            rows.append(row)
    
    return pd.DataFrame(rows)


def plot_experiment_results(experiment_dir: str, metrics: List[str] = None, 
                          methods_to_plot: Optional[List[str]] = None,
                          output_dir: Optional[str] = None) -> None:
    """Generate all plots for a DAG experiment."""
    if metrics is None:
        metrics = ['f1', 'precision', 'recall']
    
    # Load experiment data
    experiment_data = load_experiment_data(experiment_dir)
    
    if not experiment_data['results']:
        print("No results found in experiment directory")
        return
    
    # Create output directory
    if output_dir is None:
        current_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(current_dir)
        plots_dir = os.path.join(project_root, "plots")
        os.makedirs(plots_dir, exist_ok=True)
        output_dir = plots_dir
    
    # Generate plots for each metric
    exp_num = os.path.basename(experiment_dir).split('_')[-1]
    for metric in metrics:
        plot_path = os.path.join(output_dir, f"exp{exp_num}_dag_{metric}_comparison.png")
        create_violin_plot(experiment_data, metric, methods_to_plot=methods_to_plot, 
                          save_path=plot_path, show_plot=False)
    
    # Generate comparison table
    table_df = create_comparison_table(experiment_data, metrics)
    table_path = os.path.join(output_dir, f"exp{exp_num}_dag_results_table.csv")
    table_df.to_csv(table_path, index=False)
    print(f"Results table saved to: {table_path}")
    
    print(f"All plots saved to: {output_dir}")


# Example usage
if __name__ == "__main__":
    # Example usage - update experiment_dir with your actual experiment number
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(current_dir)
    exp_num = "356"
    experiment_dir = os.path.join(project_root, "results", f"experiment_{exp_num}")
    
    print(f"Looking for experiment at: {experiment_dir}")
    
    # Show where plots will be saved
    plots_dir = os.path.join(project_root, "plots")
    print(f"Plots will be saved to: {plots_dir}")
    
    if not os.path.exists(experiment_dir):
        print(f"ERROR: Experiment directory does not exist: {experiment_dir}")
        print("Available experiments:")
        results_dir = os.path.join(project_root, "results")
        if os.path.exists(results_dir):
            for exp in sorted(os.listdir(results_dir)):
                if exp.startswith("experiment_"):
                    print(f"  {exp}")
        exit()
    
    # Test DAG experiment plotting
    print("Generating DAG experiment plots...")
    
    # Load Data
    experiment_data = load_experiment_data(experiment_dir)

     # Plot single metric
    # create_violin_plot(experiment_data, "f1", show_plot=True)

    # # Plot all methods
    # create_violin_plot(experiment_data, "f1")

    # Plot specific methods only
    # Plot specific methods only and save
    # create_violin_plot(experiment_data, "f1", 
    #                 methods_to_plot=['pc_guess_expert', 'sgs_guess_expert', 'stable_pc', 
    #                                 'claude_opus_4_1_pc_guess_dag', 'claude_opus_4_1_sgs_guess_dag', 
    #                                 'claude_opus_4_1_guess_f1'],
    #                 save_path="/home/sujaih/Causality-paper-Guess-to-Graph/plots/exp356_filtered_f1_comparison.png")
    create_violin_plot(experiment_data, "f1", 
                    #    methods_to_plot=['pc','stable_pc', 'claude_opus_4_1_guess', 'claude_opus_4_1_sgs_guess_dag', 'pc_guess_expert','claude_opus_4_1_pc_guess_dag'],
                    # methods_to_plot=['pc','stable_pc', 'claude_opus_4_1_guess', 'claude_opus_4_1_pc_guess_dag', 'claude_opus_4_1_sgs_guess_dag'],
                    methods_to_plot=['pc','stable_pc', 'claude_opus_4_1_guess', 'sgs_guess_expert','claude_opus_4_1_sgs_guess_dag'],
                    # methods_to_plot=['pc', 'stable_pc', 'pc_guess_expert', 'sgs_guess_expert', 'pc_guess_dag', 'sgs_guess_dag', 'pc_guess_dag_true', 'sgs_guess_dag_true'],
                    save_path=f"/home/sujaih/Causality-paper-Guess-to-Graph/plots/llmexp_{exp_num}_filtered_f1_comparison.png")


    # # Generate all plots with method filtering
    # plot_experiment_results(experiment_dir, 
    #                     methods_to_plot=['pc', 'stable_pc', 'claude_opus_4_1_guess_f1'])

    
    # Generate all plots
    plot_experiment_results(experiment_dir)
    
    print("DAG experiment plots completed.")